# MCP (Model Context Protocol) 知識ベース

## 0. 非エンジニア向け：MCPをやさしく理解する

### USB-Cポートの比喩で理解するMCP

**MCPは「AIのためのUSB-Cポート」**です。

お手持ちのラップトップには、おそらくUSB-Cポートがついています。このポートのすごいところは、モニター、外付けハードドライブ、電源アダプターなど、**メーカーが違っても、すべて同じ規格（USB-C）で接続し、連携できる点**です。

MCPもこれと同じ役割を果たします：

1. **AIエージェントや大規模言語モデル（LLM）**: あなたのラップトップ本体
2. **MCPサーバー**: 外部データや機能を持つ周辺機器（データベース、カレンダー、ERPシステムなど）
3. **MCPプロトコル**: USB-C接続の共通規格

この共通規格のおかげで、AIエージェントは、どのシステムが提供する機能であっても、標準化された統一の方法でデータにアクセスし、アクション（行動）を実行できるようになります。

### MCPがAIエージェントに提供する3つの要素

1. **ツール (Tools)**: AIが実行できる具体的なアクション
   - 例：「顧客の請求書を作成する」「在庫を確認する」

2. **リソース (Resources)**: コンテキスト情報として提供される読み取り専用のデータ
   - 例：「ファイルの内容」「データベースのスキーマ」

3. **プロンプト (Prompts)**: あらかじめ定義された指示やテンプレート

### ビジネスパーソンにとっての4つのメリット

#### 1. 迅速なAI統合と開発コストの削減

**「一度構築すれば、何度でも統合可能」**

- 従来のAPIは、サービスごとにエンドポイントや認証スキームが異なり、AIエージェントが5つの異なるAPIを使用する場合、5つの専用アダプターが必要
- **MCPサーバーはすべて同じプロトコルで応答**するため、AIエージェント側は共通の呼び出しで連携可能
- この標準化により、開発者はAIアプリケーションやエージェントの構築や統合にかかる**時間と複雑さを軽減**

#### 2. AIエージェントの性能と信頼性の向上

**「AIが間違った行動をしないこと」が非常に重要**

- **不要な選択肢の排除**: 従来のAPIをそのまま公開すると、AIは数十〜数百もの低レベルなツールから選択することになり混乱しやすい。MCPは本当に必要な高レベルなタスクのみを公開
- **LLMに優しい情報提供**: 巨大なJSONデータではなく、構造化された形式（Markdown形式）で結果を返すことで、AIのパフォーマンス劣化を防止
- **人間らしいエラー処理**: 「次はどうすべきか」というヒントを含む応答で、AIが誤りを修正しやすくなる

#### 3. 動的な機能発見（フューチャー・プルーフ）

**「何ができますか？」と尋ねるだけで自動的に機能を発見**

- AIエージェントは、接続時にMCPサーバーに対し「あなたは何ができますか？」と尋ねるだけで、すべての機能やデータのカタログを受け取れる
- MCPサーバー側が新しい機能を追加したり、仕様を変更したりしても、**AIエージェント側のコードを書き換えたり再デプロイすることなく**、自動的に新しい機能を利用開始

#### 4. 企業データ活用の高度化

**実例：エンタープライズチャットボット**

- AIモデルが、MCPを通じて組織内の複数のデータベースに接続
- ユーザーはチャットでデータ分析を依頼でき、AIは必要な情報にアクセスしてアクション（計算、検索、データ取得など）を実行し、具体的なタスクを完了

### まとめ：AIを活用して業務効率を向上させる基盤

MCPは、AIが外部システムにアクセスするための標準化された橋をかけます。これは、AIを活用して業務効率を向上させたい企業にとって、**より迅速で、より信頼性が高く、より高性能なAIエージェントを構築するための基盤**となります。

この新しい標準を採用することで、御社のサービスをクラウドやOpenAI、ClaudeといったLLMエコシステム全体で利用可能にすることができ、ビジネスにおけるAI活用を加速させます。

---

## 1. MCPとは何か（技術詳細）

### 概要
**Model Context Protocol (MCP)** は、AIアプリケーションを外部システムに接続するためのオープンソース標準プロトコルです。AnthropicによってLLM（大規模言語モデル）が外部のデータソース、サービス、ツールと連携できるようにするために開発されました。

### 主な目的
- アプリケーションがLLMにコンテキストを提供する方法を標準化
- AIアプリケーション、LLM、外部データソース間の接続を標準化
- 動的な機能発見と文脈提供を効率的に実現

### 比喩的理解
MCPは「**AIアプリケーションのためのUSB-Cポート**」に例えられます。USB-Cが電子デバイスを接続する標準化された方法を提供するのと同様に、MCPはAIアプリケーションを外部システムに接続するための標準化された方法を提供します。

Sentry社では「**エージェントのためのプラガブルなアーキテクチャ (pluggable architecture for agents)**」として捉えています。

## 2. アーキテクチャと主要コンポーネント

### アーキテクチャ構成
MCPは**クライアント・サーバーモデル**に基づいています。

```
┌─────────────────┐
│   MCPホスト      │  例：ラップトップ、IDE
│  ┌───────────┐  │
│  │MCPクライ  │  │  ホスト内で実行され、
│  │アント     │◄─┼─ 外部MCPサーバーに接続
│  └───────────┘  │
└─────────────────┘
        ▲
        │ JSON RPC 2.0
        │ Request/Response
        ▼
┌─────────────────┐
│   MCPサーバー    │  データベース、コードリポジトリ、
│                 │  メールサーバーなど外部リソースを公開
└─────────────────┘
```

### 主要な3つのプリミティブ

MCPサーバーは以下の3つの概念をLLM（MCPクライアント）に公開します：

#### 1. Tools（ツール）
- **定義**: AIモデルが実行できる離散的なアクションや関数
- **例**:
  - APIコール
  - ファイル操作
  - ウェブ検索の実行
  - 外部サービスの呼び出し
  - 計算の実行
- **動作**: MCPクライアントがツールを呼び出すと、MCPサーバーがその基盤となる機能を実行
- **公開情報**: ツールの名前、説明、入出力スキーマ

#### 2. Resources（リソース）
- **定義**: サーバーが提供できる読み取り専用のデータ項目またはドキュメント
- **例**:
  - ドキュメント
  - ナレッジベースのエントリ
  - データベースレコード
- **特徴**: AIエージェントがオンデマンドでコンテキストデータとして取得可能

#### 3. Prompt Templates（プロンプトテンプレート）
- **定義**: 推奨されるプロンプトを提供する事前定義されたテンプレート
- **注記**: すべてのMCPサーバーがこれら3つすべてを使用するわけではなく、多くは現在ツールに焦点を当てている

### 通信方式
- **プロトコル**: JSON RPC 2.0セッション
- **パターン**: 主にRequest/Responseパターン

## 3. MCPとAPIの違い

### MCPの設計上の特徴

#### 動的ディスカバリと標準化
**MCPの最大の利点**は動的ディスカバリをサポートしている点です：

- AIエージェントは実行時にMCPサーバーに「何ができるか」を問い合わせ可能
- すべての利用可能な機能とデータの記述を受け取る
- 機械可読なカタログ（`tools/list`、`resources/list`）を公開
- コードを再デプロイすることなく新しい機能を発見・利用可能
- すべてのMCPサーバーが同じプロトコルに従い、同じパターンで応答

#### APIとの比較

| 側面 | API | MCP |
|------|-----|-----|
| **設計対象** | 人間の開発者 | LLM/AIエージェント |
| **目的** | 汎用的な接続 | AI向けに特化した接続 |
| **発見方法** | ドキュメント参照 | 動的ディスカバリ |
| **説明** | 人間向け | LLM向けに最適化 |
| **レスポンス形式** | JSON | Markdown推奨 |
| **設計思想** | リソース管理 | タスク中心 |

### なぜAPIをそのまま使えないのか

#### 1. ツール選択肢の多すぎる問題
- 一般的なAPIは75〜100のエンドポイントを持つ
- **LLMは選択肢が多すぎると混乱する**
- コンテキストサイズが大きくても、コンテキストを減らした方がはるかによく機能する

#### 2. ツール記述の最適化が必要
- 既存のAPIの説明は人間向け（Google検索で補完可能）
- **LLMはより直接的かつ明示的な記述と多くの例を必要とする**
- モデル向けに調整が必要

#### 3. タスク中心の設計が必要
- ほとんどのAPIは低レベルのリソース管理と自動化のために設計
- **LLMが必要とするのは特定の目標を達成するためのタスク、アクション、ツール**
- タスク中心のゴールを念頭に置いた設計が必要

#### 4. レスポンスの最適化が必要
- APIは通常巨大なJSONペイロードを返す
- **LLMはこれらを扱うのが得意ではない**
- Markdownのような構造化されたフォーマットで情報を返すことが推奨
- パターンマッチングによって推論しやすくなる

### MCPの役割
**MCPはAPIの上に、LLMフレンドリーなインターフェースを提供するためのレイヤー**として機能します。単なるAPIの公開ではなく、LLMが使いやすいように、ツールの選定、記述、レスポンス形式を設計し、LLMに提供するコンテキストを最適化することが重要です。

## 4. 具体的な使用例

### 開発環境とバグ修正

#### IDE連携（Sentry社の事例）
- **ツール**: Sentry MCP Server
- **環境**: VS Code Insiders、Cursor
- **機能**:
  - エディタにSentryのバグ情報を取り込む
  - エージェントがコンテキストを得てバグ修正を試行
  - 「すべてのバグを修正してほしい」という要求でエージェントがバグ修正を開始

#### コード生成とデザイン連携
- Claude CodeがFigmaデザインを使用してウェブアプリケーション全体を生成

#### データベース管理（Neon社の事例）
- Neon MCP Serverを利用
- Cursorユーザーがアプリケーション構築を依頼
- MCPサーバーがPostgresデータベースをプロビジョニング
- アプリケーションが完全に機能するように設定

### エンタープライズ・業務利用

#### パーソナライズされたAIアシスタント
- Google CalendarやNotionにアクセス
- よりパーソナライズされたAIアシスタントとして機能

#### データ分析
- エンタープライズのチャットボットが組織内の複数のデータベースに接続
- ユーザーがチャットを通じてデータを分析

#### 業務連携
- 業務アプリケーションがCRMやERPシステムなどのエンタープライズシステムと連携

#### 複雑なワークフローの委譲（A2Aとの併用）
- **事例**: 自動車修理工場システム
  - 診断エージェントがMCPで車両スキャナーや修理マニュアルDBにアクセス
  - 部品供給エージェントがA2A（Agent2Agent Protocol）で連携
- **MCPの役割**: AIモデルとツール間の接続
- **A2Aの役割**: 独立したAIエージェント間の協調
- **関係**: 相互に補完し合う

### その他の応用

#### クリエイティブな作業
- AIモデルがBlenderで3Dデザインを作成
- 3Dプリンターで出力

## 5. 実装時の課題

### 認証・セキュリティの複雑さ

#### OAuth 2.1の問題
- **課題**: MCPはOAuth 2.1を必要とするが、多くの既存システムはサポートしていない
- **Sentry社の対応**: Cloudflare Shimを使用してCloudflare Workers上にプロキシを構築

#### リモート環境での要件
- B2B SaaS企業などはリモート環境でのOAuthを特に考慮する必要

#### プロンプトインジェクションのリスク
- **警告**: 「非常に、非常に、非常に恐ろしい」問題
- **対策**: 組織内でランダムなMCPツールを許可すべきではない
- **推奨**: 信頼できる人々のパッケージのみを使用

### LLMの特性と設計の不一致

#### ツール選択肢過多（Too Much Choice）
- 既存のAPIをすべてツールとして公開すると、LLMが混乱しパフォーマンス低下
- **重要**: コンテキストを減らした方がはるかによく機能する

#### APIの説明はLLM向けではない
- 従来のAPIの説明は人間向け
- LLMはより直接的かつ明示的な記述と多くの例を必要とする

#### 巨大なJSONペイロードの処理
- 従来のAPIが返す巨大なJSONペイロードはLLMが扱うのが苦手
- モデルは自然言語で訓練・設計されている
- 大量のデータはコンテキストウィンドウを圧迫
- **「コンテキスト腐敗 (context rot)」**によりLLMのパフォーマンスを低下

#### ストリーミングの欠如
- 現在のMCPではツールに対するストリーミング応答がサポートされていない
- Agent-to-Agentの連携を考える上で大きな問題

### クライアント側の安定性と進化

#### クライアントの実装依存性
- MCPはプラグインアーキテクチャのためクライアント（VS Code、Cursor）の実装に依存
- 初期には認証サポートが安定しない、または頻繁に壊れる問題があった

#### コスト転嫁への無頓着さ
- LLMが使用するツールの呼び出しでは使用トークン量によってコストがユーザー側に転嫁
- APIをそのまま公開すると大量のトークンが必要
- ツールの呼び出しコストが大幅に増加する恐れ
- **コスト意識が必要**

## 6. ベストプラクティス

### A. LLM中心の設計原則

#### 1. タスク中心のツールの設計
- **原則**: APIは低レベルのリソース管理、MCPツールはタスク・アクション・ワークフロー中心
- **Neon社の事例**:
  - 単純な`run SQL`ツールだけでなく
  - `prepare database migration`や`complete database migration`といった複雑なマルチステップワークフローをカプセル化
  - REST APIでは意味をなさないが、エージェント環境では非常に有用

#### 2. 必要なツールのみを公開する
- APIの全エンドポイントを自動生成して公開するのではなく、LLMにとって不可欠なツールに絞り込む
- サーフェスエリア（表面積）を減らす
- **選択肢を減らすことがLLMのパフォーマンス向上につながる**

#### 3. モデルフレンドリーな説明の作成
- ツールの名前、説明、入出力スキーマをモデル向けに調整
- 具体的な例や「〜してはいけない」という指示を含める
- モデルのパフォーマンスを最大限に引き出す

#### 4. レスポンスの最適化（Markdownの使用）
- LLMは巨大なJSONペイロードを扱うのが苦手
- **Markdownなどの構造化されたフォーマットで情報を返すことが推奨**
- 人間が推論しやすい構造のため、言語モデルもパターンマッチングを通じて推論しやすい

### B. 実装と継続的な改善

#### 1. 評価（Evals）の実施とテストの反復
- LLMが正しいジョブのために正しいツールを呼び出していることを確認
- **Evals（評価）を使用してテストを繰り返し実行**
- ツールの説明を反復的に改善

#### 2. エラーメッセージの改善
- ツールからエラーが返される場合、モデルに何が問題で次に何をすべきかという手掛かりを含むエラーメッセージを返す

#### 3. 継続的な更新
- **MCPサーバーは「一度構築して終わり（set and forget）」のシステムではない**
- Sentry社は何が起こっているかを監視し、毎週更新と調整を実施

#### 4. エージェントアーキテクチャへの集中
- MCPは単なるプラグインアーキテクチャ
- **LLMがもたらす本質的な価値はエージェントアーキテクチャにある**
- MCPサーバー構築時は最終的にエージェントを構築することに焦点
- ワークフローにおけるコンテキストの最適化を考慮

### 最重要原則

> **MCPサーバーの構築は、APIをプロキシするのではなく、提供するコンテキストを最適化する作業である**

## 7. 参考資料

このドキュメントは以下のソースから情報を収集しました：

1. MCP Is Not Good Yet — David Cramer, Sentry (YouTube)
2. MCP vs API: Simplifying AI Agent Integration with External Data (YouTube)
3. MCPとA2Aプロトコルの比較と連携 (Document)
4. Your API is not an MCP | DEMFP786 (YouTube)
5. Your API ≠ MCP (avoid this mistake) (YouTube)
6. モデルコンテキストプロトコル（MCP）の概要 (Document)

---

*最終更新: 2025-10-02*
